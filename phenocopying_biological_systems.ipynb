{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151544b5",
   "metadata": {},
   "source": [
    "This script performs the kernel DMD analysis in the paper 'Kernel-DMD for multiome integration with control' by Pierides, Kramml, Waldherr and Weckwerth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c83376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  \n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from functools import reduce\n",
    "import scipy \n",
    "from itertools import product\n",
    "import random \n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(None)  # uncomment when you want to reproduce eigenmode results from one iteration -> always use seed 8\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\" \n",
    "\n",
    "from preprocessing import *\n",
    "from reconstructions import *\n",
    "from SystemIdentification_and_Control import *\n",
    "from figures import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756b838",
   "metadata": {},
   "source": [
    "Import multiome data for each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global MFsha_met, MFexp_met, Roseasha_met, Roseaexp_met\n",
    "\n",
    "Roseasha_trans = pd.read_csv(\"Roseatran_c.tsv\").iloc[:, 2:]\n",
    "Roseasha_prot = pd.read_csv(\"Roseaprot_c.tsv\").iloc[:, 2:]\n",
    "Roseasha_met = pd.read_csv(\"Roseamet_c.csv\").iloc[:, 2:]\n",
    "\n",
    "MFsha_trans = pd.read_csv(\"Majortran_c.tsv\").iloc[:, 2:]\n",
    "MFsha_prot = pd.read_csv(\"Majorprot_c.tsv\").iloc[:, 2:]\n",
    "MFsha_met = pd.read_csv(\"Majormet_c.csv\").iloc[:, 2:]\n",
    "\n",
    "Roseaexp_trans = pd.read_csv(\"Roseatran_s.tsv\").iloc[:, 2:]\n",
    "Roseaexp_prot = pd.read_csv(\"Roseaprot_s.tsv\").iloc[:, 2:]\n",
    "Roseaexp_met = pd.read_csv(\"Roseamet_s.csv\").iloc[:, 2:]\n",
    "\n",
    "MFexp_trans = pd.read_csv(\"Majortran_s.tsv\").iloc[:, 2:]\n",
    "MFexp_prot = pd.read_csv(\"Majorprot_s.tsv\").iloc[:, 2:]\n",
    "MFexp_met = pd.read_csv(\"Majormet_s.csv\").iloc[:, 2:]\n",
    "\n",
    "MFsha_prot.columns = [re.sub(\"value.\", \"\", x) for x in MFsha_prot.columns]\n",
    "Roseasha_prot.columns = [re.sub(\"value.\", \"\", x) for x in Roseasha_prot.columns]\n",
    "\n",
    "MFsha_trans.columns = [re.sub(\"value.\", \"\", x) for x in MFsha_trans.columns]\n",
    "Roseasha_trans.columns = [re.sub(\"value.\", \"\", x) for x in Roseasha_trans.columns]\n",
    "\n",
    "MFsha_trans = MFsha_trans.loc[:, (MFsha_trans != 0).any(axis=0)]\n",
    "Roseasha_trans = Roseasha_trans.loc[:, (Roseasha_trans != 0).any(axis=0)]\n",
    "\n",
    "MFsha_prot = MFsha_prot.loc[:, (MFsha_prot != 0).any(axis=0)]\n",
    "Roseasha_prot = Roseasha_prot.loc[:, (Roseasha_prot != 0).any(axis=0)]\n",
    "\n",
    "MFexp_prot.columns = [re.sub(\"value.\", \"\", x) for x in MFexp_prot.columns]\n",
    "Roseaexp_prot.columns = [re.sub(\"value.\", \"\", x) for x in Roseaexp_prot.columns]\n",
    "\n",
    "MFexp_trans.columns = [re.sub(\"value.\", \"\", x) for x in MFexp_trans.columns]\n",
    "Roseaexp_trans.columns = [re.sub(\"value.\", \"\", x) for x in Roseaexp_trans.columns]\n",
    "\n",
    "MFexp_trans = MFexp_trans.loc[:, (MFexp_trans != 0).any(axis=0)]\n",
    "Roseaexp_trans = Roseaexp_trans.loc[:, (Roseaexp_trans != 0).any(axis=0)]\n",
    "\n",
    "MFexp_prot = MFexp_prot.loc[:, (MFexp_prot != 0).any(axis=0)]\n",
    "Roseaexp_prot = Roseaexp_prot.loc[:, (Roseaexp_prot != 0).any(axis=0)]\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.3)\n",
    "sel_var = sel.fit_transform(Roseasha_trans)\n",
    "Roseasha_trans = Roseasha_trans[Roseasha_trans.columns[sel.get_support(indices=True)]]\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.3)\n",
    "sel_var = sel.fit_transform(MFsha_trans)\n",
    "MFsha_trans = MFsha_trans[MFsha_trans.columns[sel.get_support(indices=True)]]\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.3)\n",
    "sel_var = sel.fit_transform(Roseaexp_trans)\n",
    "Roseaexp_trans = Roseaexp_trans[Roseaexp_trans.columns[sel.get_support(indices=True)]]\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.3)\n",
    "sel_var = sel.fit_transform(MFexp_trans)\n",
    "MFexp_trans = MFexp_trans[MFexp_trans.columns[sel.get_support(indices=True)]]\n",
    "\n",
    "# select only common columns between species\n",
    "\n",
    "common_columns_trans = reduce(np.intersect1d, [\n",
    "    Roseasha_trans.columns,\n",
    "    MFsha_trans.columns,\n",
    "    Roseaexp_trans.columns,\n",
    "    MFexp_trans.columns\n",
    "])\n",
    "\n",
    "common_columns_prot = reduce(np.intersect1d, [\n",
    "    Roseasha_prot.columns,\n",
    "    MFsha_prot.columns,\n",
    "    Roseaexp_prot.columns,\n",
    "    MFexp_prot.columns\n",
    "])\n",
    "\n",
    "global Roseaexp_trans2, Roseasha_trans2, MFexp_trans2, MFsha_trans2, Roseaexp_prot2, Roseasha_prot2, MFexp_prot2, MFsha_prot2\n",
    "\n",
    "Roseaexp_trans2 = Roseaexp_trans[common_columns_trans]\n",
    "Roseasha_trans2 = Roseasha_trans[common_columns_trans]\n",
    "MFexp_trans2 = MFexp_trans[common_columns_trans]\n",
    "MFsha_trans2 = MFsha_trans[common_columns_trans]\n",
    "\n",
    "Roseaexp_prot2 = Roseaexp_prot[common_columns_prot]\n",
    "Roseasha_prot2 = Roseasha_prot[common_columns_prot]\n",
    "MFexp_prot2 = MFexp_prot[common_columns_prot]\n",
    "MFsha_prot2 = MFsha_prot[common_columns_prot]\n",
    "\n",
    "# import cam features\n",
    "\n",
    "met_names = pd.read_csv('met_names.csv').iloc[:, 0].values\n",
    "\n",
    "cam = pd.read_csv(\"cam_features.tsv\")\n",
    "feature_labels = pd.read_csv(\"feature_table.csv\")\n",
    "feature_labels.replace(\"NA\", np.nan, inplace=True)\n",
    "feature_labels = feature_labels[~feature_labels[\"Name\"].isna()].reset_index(drop=True)\n",
    "feature_labels = feature_labels.dropna(subset=[\"Name\", \"Pathway\"]).reset_index(drop=True)\n",
    "feature_labels = feature_labels[feature_labels[\"Name\"] != \"NA\"].reset_index(drop=True)\n",
    "\n",
    "CAM_related_pathways2 = ['Carbon fixation by Calvin cycle',  'Malate_turnover', 'Carboxylation', 'Decarboxylation', 'Pyruvate metabolism', 'Glycolysis / Gluconeogenesis', 'Starch and sucrose metabolism',\n",
    "'Circadian rhythm', 'Citrate cycle (TCA cycle)', 'Glyoxylate and dicarboxylate metabolism', 'Vacuolar_storage', 'Starch_syn/deg', 'Glycine, serine and threonine metabolism',\n",
    "'Carbon_breakdown', 'Fructose and mannose metabolism','Galactose metabolism',  'Inositol phosphate metabolism', 'Regulation', 'Fatty acid syn/deg']\n",
    "\n",
    "feature_labels = feature_labels[feature_labels['Pathway'].isin(CAM_related_pathways2)].reset_index(drop=True)\n",
    "\n",
    "cam_tran_MF_sha, cam_prot_MF_sha = get_cam_features(MFsha_trans, MFsha_prot, cam['SingleCopyOG'].values)\n",
    "cam_tran_Rosea_sha, cam_prot_Rosea_sha = get_cam_features(Roseasha_trans, Roseasha_prot, cam['SingleCopyOG'].values)\n",
    "\n",
    "cam_tran_MF_exp, cam_prot_MF_exp = get_cam_features(MFexp_trans, MFexp_prot, cam['SingleCopyOG'].values)\n",
    "cam_tran_Rosea_exp, cam_prot_Rosea_exp = get_cam_features(Roseaexp_trans, Roseaexp_prot, cam['SingleCopyOG'].values)\n",
    "\n",
    "# 0-padding of missing cam_features\n",
    "\n",
    "list1 = pd.Index(np.concatenate([cam_tran_Rosea_sha.columns, cam_tran_Rosea_exp.columns, cam_tran_MF_exp.columns])).difference(cam_tran_MF_sha.columns)\n",
    "list2 = pd.Index(np.concatenate([cam_tran_MF_sha.columns, cam_tran_MF_exp.columns, cam_tran_Rosea_exp.columns])).difference(cam_tran_Rosea_sha.columns)\n",
    "list3 = pd.Index(np.concatenate([cam_prot_Rosea_sha.columns, cam_prot_Rosea_exp.columns, cam_prot_MF_exp.columns])).difference(cam_prot_MF_sha.columns)\n",
    "list4 = pd.Index(np.concatenate([cam_prot_MF_sha.columns, cam_prot_MF_exp.columns, cam_prot_Rosea_exp.columns])).difference(cam_prot_Rosea_sha.columns)\n",
    "\n",
    "list1_1 = pd.Index(np.concatenate([cam_tran_Rosea_sha.columns, cam_tran_Rosea_exp.columns, cam_tran_MF_sha.columns])).difference(cam_tran_MF_exp.columns)\n",
    "list2_1 = pd.Index(np.concatenate([cam_tran_MF_sha.columns, cam_tran_MF_exp.columns, cam_tran_Rosea_sha.columns])).difference(cam_tran_Rosea_exp.columns)\n",
    "list3_1 = pd.Index(np.concatenate([cam_prot_Rosea_sha.columns, cam_prot_Rosea_exp.columns, cam_prot_MF_sha.columns])).difference(cam_prot_MF_exp.columns)\n",
    "list4_1 = pd.Index(np.concatenate([cam_prot_MF_sha.columns, cam_prot_MF_exp.columns, cam_prot_Rosea_sha.columns])).difference(cam_prot_Rosea_exp.columns)\n",
    "\n",
    "cam_tran_MF_sha = add_missing_columns(cam_tran_MF_sha, list1)\n",
    "cam_prot_MF_sha = add_missing_columns(cam_prot_MF_sha, list3)\n",
    "cam_tran_Rosea_sha = add_missing_columns(cam_tran_Rosea_sha, list2)\n",
    "cam_prot_Rosea_sha = add_missing_columns(cam_prot_Rosea_sha, list4)\n",
    "\n",
    "cam_tran_MF_exp = add_missing_columns(cam_tran_MF_exp, list1_1)\n",
    "cam_prot_MF_exp = add_missing_columns(cam_prot_MF_exp, list3_1)\n",
    "cam_tran_Rosea_exp = add_missing_columns(cam_tran_Rosea_exp, list2_1)\n",
    "cam_prot_Rosea_exp = add_missing_columns(cam_prot_Rosea_exp, list4_1)\n",
    "\n",
    "# phenotypes\n",
    "\n",
    "# major\n",
    "\n",
    "Malate_MF_sha = MFsha_met.iloc[:, pd.DataFrame(np.where(MFsha_met.columns.isin(['Malic_acid']))).T[0].values.tolist()]\n",
    "Malate_MF_exp = MFexp_met.iloc[:, pd.DataFrame(np.where(MFexp_met.columns.isin(['Malic_acid']))).T[0].values.tolist()]\n",
    "\n",
    "MFgas = pd.read_csv(\"MFgas.csv\")\n",
    "\n",
    "MFsha_prot.columns = [re.sub(\"values.\", \"\", x) for x in MFsha_prot.columns]\n",
    "MFexp_prot.columns = [re.sub(\"values.\", \"\", x) for x in MFexp_prot.columns]\n",
    "\n",
    "CAM_pheno_M_sha = MFsha_prot.iloc[:, pd.DataFrame(np.where(MFsha_prot.columns.isin(['OG0002610::H1', 'OG0001285::H1', 'OG0004358::H2', 'OG0001386::H1']))).T[0].values.tolist()]  # 'OG0001285::H1' PPC1, 'OG0004358::H2' PHO1, 'OG0001386::H1' PPD\n",
    "\n",
    "CAM_pheno_M_exp = MFexp_prot.iloc[:, pd.DataFrame(np.where(MFexp_prot.columns.isin(['OG0002610::H1', 'OG0001285::H1', 'OG0004358::H2', 'OG0001386::H1']))).T[0].values.tolist()]  # 'OG0001285::H1' PPC1,'OG0004358::H2' PHO1, 'OG0001386::H1' PPD\n",
    "\n",
    "# rosea\n",
    "\n",
    "Malate_Rosea_sha = Roseasha_met.iloc[:, pd.DataFrame(np.where(Roseasha_met.columns.isin(['Malic_acid']))).T[0].values.tolist()]\n",
    "Malate_Rosea_exp = Roseaexp_met.iloc[:, pd.DataFrame(np.where(Roseaexp_met.columns.isin(['Malic_acid']))).T[0].values.tolist()]\n",
    "\n",
    "Roseagas = pd.read_csv(\"Roseagas.csv\")\n",
    "\n",
    "Roseasha_prot.columns = [re.sub(\"values.\", \"\", x) for x in Roseasha_prot.columns]\n",
    "Roseaexp_prot.columns = [re.sub(\"values.\", \"\", x) for x in Roseaexp_prot.columns]\n",
    "\n",
    "CAM_pheno_R_sha = Roseasha_prot.iloc[:, pd.DataFrame(np.where(Roseasha_prot.columns.isin(['OG0002610::H1', 'OG0001285::H1', 'OG0004358::H2', 'OG0001386::H1']))).T[0].values.tolist()]  # PEPC1, PCK1, PHO1, PPD\n",
    "\n",
    "CAM_pheno_R_exp = Roseaexp_prot.iloc[:, pd.DataFrame(np.where(Roseaexp_prot.columns.isin(['OG0002610::H1', 'OG0001285::H1', 'OG0004358::H2', 'OG0001386::H1']))).T[0].values.tolist()]  # PEPC1, PCK1, PHO1, PPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f308cbf",
   "metadata": {},
   "source": [
    "SYSTEM IDENTIFICATION\n",
    "\n",
    "Next, within the eval_kernel function 4 different types of kernels are evaluated for how well their respective Koopman operator outputs reconstruct the data and the phenotype data. The sigmoidal kernel was chosen for final data analysis due to its resemblance to Hill function dynamics and the use of a-priori molecular pathway information. In the eval_kernel function the process_kernel_grid_item function performs the following:\n",
    "\n",
    "calculates the kernel and its gradient of each species and condition\n",
    "multi-objective function to map inputs forward in time and to phenotypic outputs\n",
    "linear and nonlinear disambiguation of dynamics\n",
    "eigendecomposition\n",
    "modal, data and output reconstructions\n",
    "saves a file with node feature information including eigenmode ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f10858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kernel_grid_item(i, j, val, grid_row, species, features, names, products, features2):\n",
    "        # Kernel computation logic for a single (i, val, grid_row) combination\n",
    "        random.seed(None) #uncomment for replication of one iteration results seen in the paper \n",
    "        np.random.seed(None)\n",
    "        kernel_outputs = {\"params\": [], \"K_tilde\": [], \"eVals\": [], \"modes\": [], \"mode pairs\": [],\n",
    "                        \"mode_amplitudes\": [], \"gradients\": [], \"Q\": [], \"R\": [], \"U\": [], \"C\": [], \"W\": [],\n",
    "                        \"V\": [], \"B\": [], \"C_full\": [],\"C_exo\": [],\"inputs\": [], \"exo_inputs\": [], \"man_inputs\": [], \"man_inputs_index\": [], \"V_man\": [], \"V_exo\": [], \n",
    "                        \"spectral_cluster\": [], \"data_error\": [], \"W_man\": [], \"W_exo\": [], \"controllable\": [], \n",
    "                        \"output_error\": [], \"features\": [], \"names\": [], \"condition number\": [], \"r2_data_no_c\": [], \"inputs_t\": [],\n",
    "                        \"s\": [], \"eigs_in\": [], \"r2_data_full_model\": [], \"cd_outs_full_model\": [], \"adj_matrix\": [],  \n",
    "                        \"K_full\": [], \"K_exo\": [], \"recon\": [], \"recon_modal\": [], \"r2_modal\": [], \"CO2_ind\": [], \"obs_rank\": [], \"modal_dictionary\": [],\n",
    "                        \"cd_outs\": [], \"r2_data\": [], \"n_modes\": [], \"coef2\": [], \"beta\": []}\n",
    "        \n",
    "        param = grid_row\n",
    "        inputs = np.concatenate((val[0], val[1], val[2]), axis=1)\n",
    "\n",
    "        inputs_t = np.concatenate((val[6], val[7], val[8]), axis=1)\n",
    "        ntps = inputs.shape[0]\n",
    "        np.set_printoptions(precision=16)\n",
    "        \n",
    "        # Kernel training\n",
    "        beta = - param[4] * np.mean(np.dot(inputs, inputs.T))\n",
    "        coef2 = 0.1 * np.var(inputs)\n",
    "        adj_matrix = make_int_matrix(names)\n",
    "\n",
    "        grad1, kernOut1 = kernel_gradient(\n",
    "                kernel=param[0], inputs=inputs,\n",
    "                gamma=param[1], coef0=param[2],\n",
    "                degree=param[3], gamma1=param[4],\n",
    "                delta1=beta, coef2=coef2,\n",
    "                coef3=param[7], adj_matrix=adj_matrix\n",
    "            )\n",
    "            \n",
    "        kernOut1 += 1e-3 * np.eye(kernOut1.shape[0])\n",
    "        U, s, V = scipy.linalg.svd(grad1.T, lapack_driver='gesvd')\n",
    "        energy = np.cumsum(s**2) / np.sum(s**2)\n",
    "\n",
    "        # Choose rank where 95% of the energy is retained\n",
    "        threshold = 0.99\n",
    "        rank = np.searchsorted(energy, threshold) + 1\n",
    "        \n",
    "        U = U[:, :rank]\n",
    "        V = V.conj().T[:, :rank]\n",
    "        s = s[:rank]\n",
    "        \n",
    "        W1 = cp.Variable((inputs.shape[1], ntps))\n",
    "\n",
    "        C = cp.Variable((n_phenos, ntps))\n",
    "        K_tilde = U.T @ W1 @ grad1 @ U\n",
    "        constraints = [cp.norm(K_tilde, 2) - 1 <= 0.001]\n",
    "\n",
    "        outs = val[3].T\n",
    "        outs_t = val[4].T\n",
    "\n",
    "        alpha = 1\n",
    "        obj = cp.Minimize(\n",
    "                cp.square(cp.norm(inputs_t.T - W1 @ kernOut1, \"fro\")) +\n",
    "                alpha * (1 - 0.99 ) * cp.norm(W1, \"fro\") + alpha * 0.99 * cp.mixed_norm(W1, 2, 1) +\n",
    "                cp.square(cp.norm(outs - C @ kernOut1, \"fro\")) +\n",
    "                alpha * (1 - 0.99 ) * cp.norm(C, \"fro\") + alpha * 0.99 * cp.mixed_norm(C, 2, 1)\n",
    "            )\n",
    "        prob = cp.Problem(obj)\n",
    "        prob.solve(solver='CLARABEL')\n",
    "        W1 = W1.value\n",
    "        C = C.value\n",
    "    \n",
    "        C_full = C @ grad1\n",
    "        K_full = W1 @ grad1 \n",
    "        W = W1\n",
    "                \n",
    "        # nonlinear operator\n",
    "        inputs2 = inputs.copy()\n",
    "        inputs2_t = inputs_t.copy()\n",
    "            \n",
    "        # bias is base state \n",
    "        bias = np.mean(inputs2, axis = 1)\n",
    "            \n",
    "        beta = - param[4] *  np.mean(np.dot(bias[:, np.newaxis], bias[:, np.newaxis].T)) \n",
    "        coef2 = 0.1 * np.var(inputs)\n",
    "        _, kernOutbias = kernel_gradient(kernel=param[0], inputs=bias[:, np.newaxis], \n",
    "                                                gamma=1e-3, coef0=param[2],\n",
    "                                                degree=param[3], gamma1=param[4], delta1=beta, coef2=0.1 * np.var(bias[:, np.newaxis]),\n",
    "                                                coef3=param[7], adj_matrix=adj_matrix)\n",
    "            \n",
    "        kernOutbias = kernOutbias + 1e-3 * np.eye(kernOutbias.shape[0])\n",
    "\n",
    "        fluctuations = inputs - bias[:, np.newaxis]\n",
    "        # N is nonlinear residual term of Taylor series (full model - bias - linear dynamics)\n",
    "        N = W.dot(kernOut1 + kernOutbias) - W.dot(kernOutbias) - np.linalg.multi_dot([W, grad1, inputs.T.reshape(-1, inputs.T.shape[-1]),])\n",
    "       # N = W.dot(kernOut1) - 0 - np.linalg.multi_dot([W, grad1, inputs.T.reshape(-1, inputs.T.shape[-1]),])\n",
    "        # N = Bu\n",
    "        u, s2, v = np.linalg.svd(N, full_matrices=False) # SVD of nonlinear forcing parts\n",
    "\n",
    "        nonzero_inds = np.abs(s2) > 1e-16\n",
    "        s2 = s2[nonzero_inds]\n",
    "\n",
    "        sorted_inds = np.argsort(s2)[::-1]\n",
    "        s2 = s2[sorted_inds]\n",
    "        u = u[:, sorted_inds]\n",
    "        N = N[sorted_inds, :]\n",
    "        \n",
    "        # keep only high singular values for nonlinear parts\n",
    "        retained = np.argmax(np.cumsum(s2**2) / np.sum(s2**2) >= 0.9) + 1 # choose based on 'energy' of singular values        \n",
    "\n",
    "        u_manipulated = u[:, :retained] # B\n",
    "        s2_man = s2[:retained]\n",
    "        Vh_r = v[:retained, :]\n",
    "        u_exogenous = u[:, retained:]\n",
    "\n",
    "        exogenous_inputs = inputs # np.dot(inputs, u_exogenous)  \n",
    "        # np.dot(inputs, u_manipulated) \n",
    "        s2_man = np.maximum(s2[:retained], 1e-2) \n",
    "        B = u_manipulated @ np.diag(s2_man)         # shape: (n_outputs, r)\n",
    "        manipulated_inputs = Vh_r # np0diag(s2_man) @ Vh_r  # inputs u from nonlinear N decomposition right singular vectors\n",
    "        manipulated_inputs = manipulated_inputs.T\n",
    "\n",
    "        K_tilde = K_tilde.value \n",
    "\n",
    "        # eigendecomposition       \n",
    "        eigenvalues, eigenvectors = np.linalg.eig(K_tilde)\n",
    "        nonzero_inds = np.abs(eigenvalues) > 1e-6\n",
    "        eigenvalues = eigenvalues[nonzero_inds]\n",
    "        eigenvectors = eigenvectors[:, nonzero_inds]\n",
    "\n",
    "        # Sort eigenvalues, descending based on modulus.\n",
    "        sorted_inds = np.argsort(-np.real(eigenvalues))\n",
    "        eVals_r = eigenvalues[sorted_inds]\n",
    "        eVecs_r = eigenvectors[:, sorted_inds]\n",
    "\n",
    "        # eigenmodes      \n",
    "        Phi = np.linalg.multi_dot([W1, V, np.diag(s), eVecs_r, np.diag(1 / eVals_r), ])\n",
    "        Phi = Phi / np.linalg.norm(Phi, axis=0)   \n",
    "        \n",
    "        mode_pairs = []\n",
    "        cnt = 0\n",
    "        while cnt in range(len(eVals_r)):\n",
    "            if cnt + 1 != len(eVals_r):\n",
    "                if np.abs(eVals_r[cnt]) == np.abs(eVals_r[cnt + 1]):\n",
    "                    mode_pairs.append([cnt, cnt + 1])\n",
    "                    cnt += 2\n",
    "                else:\n",
    "                    mode_pairs.append([cnt])\n",
    "                    cnt += 1\n",
    "            else:\n",
    "                mode_pairs.append([cnt])\n",
    "                cnt += 1\n",
    "        \n",
    "        clean_name = re.sub(r'[^\\w\\s-]', '', list(species)[j])  # Remove any non-word, non-space, and non-hyphen characters\n",
    "        clean_name = clean_name.replace(\" \", \"_\")\n",
    "        \n",
    "        kmax = 90 \n",
    "        kmax_inds_list = []\n",
    "        for ii in range(Phi.shape[1]):\n",
    "            ind = Phi[:, ii].argsort()[-kmax:]\n",
    "            kmax_inds_list.append(ind[:30])\n",
    "         #   mask = [i for i in range(Phi.shape[0]) if i not in ind]\n",
    "         #   Phi[mask, ii] = 0\n",
    "                \n",
    "        max_norm = 60\n",
    "       #b_r1 = np.linalg.pinv(Phi) @ fluctuations.T\n",
    "        b_r1 = cp.Variable((Phi.shape[1], inputs.shape[0]))\n",
    "        constraints = [cp.norm(b_r1, 2) <= max_norm]\n",
    "        obj1 = cp.Minimize(cp.norm(outs_t - C_full @ Phi @ np.diag(eVals_r) @ b_r1, \"fro\")) #+ 0.01 * cp.norm(b_r1, 1))\n",
    "        prob1 = cp.Problem(obj1, constraints)\n",
    "        prob1.solve(solver='CLARABEL')\n",
    "        b_r1 = b_r1.value\n",
    "        \n",
    "        # reconstructions \n",
    "        recon_modal, cd_modal, r2_modal, modal_error = modal_reconstruction(C=C_full, outs=outs, eVals_r=eVals_r, modes=Phi, b_r0=b_r1, ntimepts=12, nmodes=len(mode_pairs), n_phenos=n_phenos, mode_pairs=mode_pairs)\n",
    "        CO2_ind = []\n",
    "        for j in range(len(r2_modal)):\n",
    "            if r2_modal[j][2] > 0.3:\n",
    "                CO2_ind.append(j)\n",
    "                \n",
    "        recon, cd_outs, mse1, r2_outs, error_out = reconstructed_outputs(K=K_full, C=C_full, outs=val[3].T, inputs=inputs, ntimepts=12, n_phenos=n_phenos)\n",
    "        \n",
    "        snapshots, cd_data, mse, r2_data = reconstructed_data(K=K_full, inputs1=inputs, ntimepts=21)      \n",
    "\n",
    "        # obserability and network clustering                      \n",
    "        if all(np.abs(eVals_r) <= 1):\n",
    "            eigs_in = True\n",
    "        else:\n",
    "            eigs_in = False               \n",
    "        \n",
    "        '''\n",
    "        # network file\n",
    "        df = pd.DataFrame(K_full)\n",
    "        df.index.name = 'Target'\n",
    "        edges = df.reset_index().melt('Target', value_name='Weight', var_name='Source').query('Source != Target')\n",
    "        edges['Weight'] = np.real(edges['Weight'])\n",
    "        edges['Source_names'] = np.asarray(names)[edges['Source'].values.tolist()]\n",
    "        edges['Source_Prod'] = np.asarray(products)[edges['Source'].values.tolist()]\n",
    "        edges['Target_names'] = np.asarray(names)[edges['Target'].values.tolist()]\n",
    "        edges['Target_Prod'] = np.asarray(products)[edges['Target'].values.tolist()]\n",
    "        edges = edges.sort_values(by='Weight', ascending=False).iloc[:500, :]\n",
    "\n",
    "        edges.to_csv(f'{clean_name}_sigmoid.csv')\n",
    "        '''\n",
    "        \n",
    "        ## leave-3-out cross validation \n",
    "        \n",
    "        #  cross_validation(inputs=inputs, inputs_t=inputs_t, n_phenos=6, rank=rank, Pheno=outs, param0=param[0], param1=param[0], param2=param[2], param3=param[3], param4=param[4], param7=param[7])\n",
    "        \n",
    "        kernel_outputs[\"params\"].append(param)\n",
    "        # eigen outputs\n",
    "        kernel_outputs[\"eVals\"].append(eVals_r)\n",
    "        kernel_outputs[\"modes\"].append(Phi)\n",
    "        kernel_outputs[\"mode pairs\"].append(mode_pairs)\n",
    "        kernel_outputs[\"mode_amplitudes\"].append(b_r1)\n",
    "        # decomposed outputs\n",
    "        kernel_outputs[\"gradients\"].append(grad1)\n",
    "        kernel_outputs[\"U\"].append(U)\n",
    "        kernel_outputs[\"V\"].append(V)\n",
    "        kernel_outputs[\"s\"].append(s)\n",
    "        # operators\n",
    "        kernel_outputs[\"C_full\"].append(C_full)\n",
    "        kernel_outputs[\"K_full\"].append(K_full)\n",
    "        kernel_outputs[\"B\"].append(B)\n",
    "        # accuracies\n",
    "        kernel_outputs[\"recon\"].append(recon)\n",
    "        kernel_outputs[\"recon_modal\"].append(recon_modal)\n",
    "        kernel_outputs[\"r2_modal\"].append(r2_modal)\n",
    "        kernel_outputs[\"cd_outs\"].append(r2_outs)\n",
    "        kernel_outputs[\"r2_data\"].append(r2_data) #{\"R\\u00b2\": r2_data, \"corr_coef\": cd_data, \"mse\": mse})\n",
    "        kernel_outputs[\"CO2_ind\"].append(CO2_ind) #{\"R\\u00b2\": r2_data, \"corr_coef\": cd_data, \"mse\": mse})\n",
    "\n",
    "        # feature types\n",
    "        kernel_outputs[\"n_modes\"].append(rank)\n",
    "        kernel_outputs[\"eigs_in\"].append(eigs_in)\n",
    "        kernel_outputs[\"inputs\"].append(inputs)\n",
    "        kernel_outputs[\"inputs_t\"].append(inputs_t)\n",
    "        kernel_outputs[\"man_inputs\"].append(manipulated_inputs)\n",
    "        kernel_outputs[\"V_exo\"].append(u_exogenous)\n",
    "        kernel_outputs[\"V_man\"].append(u_manipulated)        \n",
    "        kernel_outputs[\"coef2\"].append(coef2)\n",
    "        kernel_outputs[\"beta\"].append(beta)\n",
    "        kernel_outputs[\"adj_matrix\"].append(adj_matrix)\n",
    "        \n",
    "        return kernel_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f782a",
   "metadata": {},
   "source": [
    "SYSTEM CONTROL\n",
    "\n",
    "In the eval_kernel function the grid results are saved for each kernel and then the control strategy follows.\n",
    "\n",
    "Firstly data reduction is carried out using Hankel Singular Values coming from the Hankel matrix (product of observability and controlability Gramians). System reidentification is carried out on the reduced features space for more efficient application of control. This follows the same kernel DMD strategy as above.\n",
    "\n",
    "The two systems are aligned using a transformation matrix T.\n",
    "\n",
    "Finally linear Model Predictive Control is carried out to match the outputs of one species to the ones of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_kernel(input_X1, input_X2, n_modes, input_X3, Pheno, Pheno_t, outputs, input_Y1, input_Y2, input_Y3, input_df1,\n",
    "                input_df2, input_df3, comparison, iter, products, names, features, features2):\n",
    "    global n_phenos\n",
    "    global rank\n",
    "    rank = n_modes\n",
    "    n_phenos = Pheno[0].shape[1]\n",
    "    np.random.seed(None)\n",
    "    random.seed(None)\n",
    "    param_linear = {'kernel': [\"linear\"], 'gamma': [0.0001], 'coef': [1], 'degree': [2],  'gamma1': [0.0001], 'delta1': [3], 'coef2': [0.1], 'coef3': [1], 'l': [3], 'retained': [3]}\n",
    "\n",
    "    param_rbf = {'kernel': [\"rbf\"], 'gamma': [0.001], 'coef': [1], 'degree': [2], 'gamma1': [0.0001], 'delta1': [3], 'coef2': [0.1], 'coef3': [1], 'l': [3], 'retained': [3]}\n",
    "\n",
    "    param_poly = {'kernel': [\"poly\"], 'gamma': [0.001], 'coef': [1], 'degree': [5], 'gamma1': [0.0001], 'delta1': [3], 'coef2': [0.1], 'coef3': [1], 'l': [3], 'retained': [3]}\n",
    "    \n",
    "    param_sigmoid = {'kernel': [\"sigmoid\"], 'gamma': [0.0001], 'coef': [1], 'degree': [3], 'gamma1': [0.001], 'delta1': [2], 'coef2': [0.1], 'coef3': [3], 'l': [3], 'retained': [3]}\n",
    "\n",
    "    param_grid3 = {\n",
    "        r'$\\it{C.\\ major}$ (control)': [input_X1[0], input_X2[0], input_X3[0], Pheno[0], Pheno_t[0], outputs[0], input_Y1[0], input_Y2[0], input_Y3[0], input_df1[0], input_df2[0], input_df3[0]],\n",
    "        r'$\\it{C.\\ major}$ (stress)': [input_X1[1], input_X2[1], input_X3[1], Pheno[1], Pheno_t[1], outputs[1], input_Y1[1],  input_Y2[1], input_Y3[1], input_df1[1], input_df2[1], input_df3[1]],\n",
    "        r'$\\it{C.\\ rosea}$ (control)': [input_X1[2], input_X2[2], input_X3[2], Pheno[2], Pheno_t[2], outputs[2], input_Y1[2], input_Y2[2], input_Y3[2], input_df1[2], input_df2[2], input_df3[2]],\n",
    "        r'$\\it{C.\\ rosea}$ (stress)': [input_X1[3], input_X2[3], input_X3[3], Pheno[3], Pheno_t[3], outputs[3], input_Y1[3], input_Y2[3], input_Y3[3], input_df1[3], input_df2[3], input_df3[3]]}\n",
    "    \n",
    "    grid0 = pd.DataFrame(product(*param_linear.values()), columns=list(param_linear.keys()))\n",
    "    grid1 = pd.DataFrame(product(*param_rbf.values()), columns=list(param_rbf.keys()))\n",
    "    grid2 = pd.DataFrame(product(*param_poly.values()), columns=list(param_poly.keys()))\n",
    "    grid3 = pd.DataFrame(product(*param_sigmoid.values()), columns=list(param_sigmoid.keys()))\n",
    "\n",
    "    grid = grid3 # for computational efficiency try each kernel separately; or try pd.concat((grid0, grid1, grid2, grid3), axis=0)\n",
    "    grid = grid.reset_index(drop=True)\n",
    "\n",
    "    species_kernels = {\"species\": [], \"kernel_outputs\": []}\n",
    "\n",
    "    r2_data_cols = [[] for _ in range(4)]\n",
    "    cd_outs_cols = [[] for _ in range(4)]\n",
    "    condition_num_cols = [[] for _ in range(4)]\n",
    "    modal_dictionaries = [[] for _ in range(4)]\n",
    "    \n",
    "    for i in range(grid.shape[0]):\n",
    "        for j, val in enumerate(param_grid3.values()):\n",
    "            kernel_outs = process_kernel_grid_item(\n",
    "                i, j, val, grid.iloc[i, :], param_grid3.keys(), features, names, products, features2\n",
    "            )\n",
    "\n",
    "            # Append metrics to the correct list per species-condition\n",
    "            r2_data_cols[j].append(kernel_outs[\"r2_data\"])\n",
    "            modal_dictionaries[j].append(kernel_outs[\"modal_dictionary\"])\n",
    "            cd_outs_cols[j].append(kernel_outs[\"cd_outs\"])\n",
    "            species_kernels[\"kernel_outputs\"].append(kernel_outs)\n",
    "    \n",
    "    species_kernels[\"species\"].append(list(param_grid3.keys()))\n",
    "\n",
    "    grid['r2_data multi (control)'] = r2_data_cols[0]\n",
    "    grid['r2_data multi (stress)'] = r2_data_cols[1]\n",
    "    grid['r2_data rosea (control)'] = r2_data_cols[2]\n",
    "    grid['r2_data rosea (stress)'] = r2_data_cols[3]\n",
    "\n",
    "    grid['Accuracy of outputs multi (control)'] = cd_outs_cols[0]\n",
    "    grid['Accuracy of outputs multi (stress)'] = cd_outs_cols[1]\n",
    "    grid['Accuracy of outputs rosea (control)'] = cd_outs_cols[2]\n",
    "    grid['Accuracy of outputs rosea (stress)'] = cd_outs_cols[3]\n",
    "    \n",
    "    grid['eigs in multi (control)'] = species_kernels['kernel_outputs'][0][\"eigs_in\"][0]\n",
    "    grid['eigs in multi (stress)'] = species_kernels['kernel_outputs'][1][\"eigs_in\"][0]\n",
    "    grid['eigs in rosea (control)'] = species_kernels['kernel_outputs'][2][\"eigs_in\"][0]\n",
    "    grid['eigs in rosea (stress)'] = species_kernels['kernel_outputs'][3][\"eigs_in\"][0]\n",
    "\n",
    "    # grid.to_csv(f\"all_kernel_evals.csv\")    # save reconstruction accuracies  \n",
    "    \n",
    "    # mode clusters figure (eigenvalues and mode amplitudes)\n",
    "    col_ind1, col_ind2 = mode_clusters_fig(eVals_1=species_kernels['kernel_outputs'][0][\"eVals\"][0],\n",
    "                        eVals_4=species_kernels['kernel_outputs'][3][\"eVals\"][0],\n",
    "                        mode_pairs1=species_kernels['kernel_outputs'][0][\"mode pairs\"][0],\n",
    "                        mode_pairs4=species_kernels['kernel_outputs'][3][\"mode pairs\"][0],\n",
    "                        b_1=species_kernels['kernel_outputs'][0][\"mode_amplitudes\"][0],\n",
    "                        b_4=species_kernels['kernel_outputs'][3][\"mode_amplitudes\"][0],\n",
    "                        label1=species_kernels[\"species\"][0][0],\n",
    "                        label4=species_kernels[\"species\"][0][3],\n",
    "                        figlabel=\"sigmoid\")\n",
    "    \n",
    "    # phenotype reconstruction figure \n",
    "    CO1_ind, CO2_ind = pheno_recons_fig(label1=species_kernels[\"species\"][0][0], label2=species_kernels[\"species\"][0][2], \n",
    "                    Pheno1=Pheno[0], Pheno2=Pheno[3], \n",
    "                    recon1=species_kernels['kernel_outputs'][0][\"recon\"][0], recon2=species_kernels['kernel_outputs'][3][\"recon\"][0], \n",
    "                    cd1=species_kernels['kernel_outputs'][0][\"cd_outs\"][0], cd2=species_kernels['kernel_outputs'][3][\"cd_outs\"][0], \n",
    "                    recon_modal1=species_kernels['kernel_outputs'][0][\"recon_modal\"][0], recon_modal2=species_kernels['kernel_outputs'][3][\"recon_modal\"][0],\n",
    "                    cd_modes1=species_kernels['kernel_outputs'][0][\"r2_modal\"][0], cd_modes2=species_kernels['kernel_outputs'][3][\"r2_modal\"][0], col_ind1=col_ind1, col_ind2=col_ind2)\n",
    "    \n",
    "    \n",
    "    # eigenmode consensus network\n",
    "    G1, G2 = modes_heatmap(Phi1= species_kernels['kernel_outputs'][0][\"modes\"][0],\n",
    "                mode_pairs1=species_kernels['kernel_outputs'][0 ][\"mode pairs\"][0],\n",
    "                Phi4=species_kernels['kernel_outputs'][3][\"modes\"][0],\n",
    "                evals1 = species_kernels['kernel_outputs'][0][\"eVals\"][0], \n",
    "                evals4 = species_kernels['kernel_outputs'][3][\"eVals\"][0], \n",
    "                mode_pairs4=species_kernels['kernel_outputs'][3][\"mode pairs\"][0],\n",
    "                names=features2, adj_matrix=species_kernels['kernel_outputs'][3][\"adj_matrix\"][0], products=products, col_ind1=col_ind1, col_ind2=col_ind2, \n",
    "                CO2_ind1=CO1_ind, CO2_ind2=CO2_ind)\n",
    "    \n",
    "    # PART 2: SYSTEM CONTROL \n",
    "\n",
    "    # System reduction via HSV ranking \n",
    "    \n",
    "    important_states1, hsv1 = gramian_hsv_ranking(species_kernels['kernel_outputs'][0][\"K_full\"][0], species_kernels['kernel_outputs'][0][\"B\"][0], species_kernels['kernel_outputs'][0][\"C_full\"][0] , k=50)\n",
    "    important_states2, hsv2 = gramian_hsv_ranking(species_kernels['kernel_outputs'][3][\"K_full\"][0], species_kernels['kernel_outputs'][3][\"B\"][0] , species_kernels['kernel_outputs'][3][\"C_full\"][0] , k=50)\n",
    "\n",
    "    names = np.asarray(names)\n",
    "    features = np.asarray(features)\n",
    "    features2 = np.asarray(features2)\n",
    "    keep_states = np.unique(np.concatenate([important_states1, important_states2]))\n",
    "    \n",
    "    red_feat_space1 = pd.DataFrame(features2[important_states1])\n",
    "    red_feat_space1.columns = [\"Feature\"]\n",
    "    red_feat_space1['hsv_value'] = hsv1\n",
    "    \n",
    "    red_feat_space2 = pd.DataFrame(features2[important_states2])\n",
    "    red_feat_space2.columns = [\"Feature\"]\n",
    "    red_feat_space2['hsv_value'] = hsv2\n",
    "\n",
    "    names_kept = names[keep_states]\n",
    "    # System reidentification in reduced feature space\n",
    "    K_A, B1, C_A = system_reIdendification(inputs=species_kernels['kernel_outputs'][0][\"inputs\"][0] [:, keep_states], inputs_t=species_kernels['kernel_outputs'][0][\"inputs_t\"][0][:, keep_states], coef=0.001, adj_matrix_kept=make_int_matrix(names_kept), n_phenos=Pheno[0].shape[1], Pheno=Pheno[0], Pheno_t=Pheno_t[0])\n",
    "    K_B, B2, C_B = system_reIdendification(inputs=species_kernels['kernel_outputs'][2][\"inputs\"][0] [:, keep_states], inputs_t=species_kernels['kernel_outputs'][2][\"inputs_t\"][0][:, keep_states], coef=0.001, adj_matrix_kept=make_int_matrix(names_kept), n_phenos=Pheno[3].shape[1], Pheno=Pheno[3], Pheno_t=Pheno_t[3])\n",
    "\n",
    "    # system alignment and projection via matrix T\n",
    "    \n",
    "    T = cp.Variable((K_A.shape[1], K_A.shape[0]))\n",
    "\n",
    "    state_error = 0\n",
    "    \n",
    "    state_error = cp.square(cp.norm(C_B @ K_B - C_A @ K_A @ T, \"fro\"))\n",
    "\n",
    "    reg_term = cp.norm(T, \"fro\") ** 2\n",
    "    obj = cp.Minimize(state_error + 0.001 * reg_term)\n",
    "    prob = cp.Problem(obj)\n",
    "    prob.solve()\n",
    "    T = T.value\n",
    "    \n",
    "    # linear Model Predictive Control \n",
    "\n",
    "    x_inputs, u_inputs, control_sensitivity, cd = linearMPC(B1, C_A, K_A @ T, species_kernels['kernel_outputs'][0][\"inputs\"][0] [:, keep_states], species_kernels['kernel_outputs'][2][\"inputs\"][0] [:, keep_states], \n",
    "                                                            Pheno1=Pheno[0], Pheno2=Pheno[3], n_phenos=n_phenos, n_controls=B1.shape[1], features_kept=names_kept, with_fig=False)\n",
    "    \n",
    "   \n",
    "    return grid, control_sensitivity, G1, G2, cd, red_feat_space1, red_feat_space2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d299289",
   "metadata": {},
   "source": [
    "The following (run iteration) performs one iteration of the algorithm by PCA data reduction, combines transcriptome and proteome data types with CAM relevant features, rerraranges the timepoints after adding a small random noise and prepares the data for input into the eval_kernel function which evaluates the performance of different types of kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(h, labels):\n",
    "        # Generate data with added noise using the grouped data\n",
    "        np.random.seed(None) # set a constant number in every seed for reproducible results \n",
    "        random.seed(None)\n",
    "        global cam_tran_MF_sha, cam_tran_MF_exp, cam_prot_MF_sha, cam_prot_MF_exp, cam_tran_Rosea_sha, cam_tran_Rosea_exp, cam_prot_Rosea_sha, cam_prot_Rosea_exp\n",
    "        global Roseaexp_trans2, Roseasha_trans2, MFexp_trans2, MFsha_trans2, Roseaexp_prot2, Roseasha_prot2, MFexp_prot2, MFsha_prot2\n",
    "        global MFsha_met, MFexp_met, Roseasha_met, Roseaexp_met\n",
    "\n",
    "        MFsha_trans2 = MFsha_trans2.iloc[:, MFsha_trans2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        MFsha_prot2 = MFsha_prot2.iloc[:, MFsha_prot2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        Roseasha_trans2 = Roseasha_trans2.iloc[:, Roseasha_trans2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        Roseasha_prot2 = Roseasha_prot2.iloc[:, Roseasha_prot2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        MFexp_trans2 = MFexp_trans2.iloc[:, MFexp_trans2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        MFexp_prot2 = MFexp_prot2.iloc[:, MFexp_prot2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        Roseaexp_trans2 = Roseaexp_trans2.iloc[:, Roseaexp_trans2.columns.isin(labels[\"SingleCopyOG\"])]        \n",
    "        Roseaexp_prot2 = Roseaexp_prot2.iloc[:, Roseaexp_prot2.columns.isin(labels[\"SingleCopyOG\"])]        #\n",
    "\n",
    "        transcriptome_concat_MF_sha = scale_and_transform(MFsha_trans2, scaler2)\n",
    "        proteome_concat_MF_sha = scale_and_transform(MFsha_prot2, scaler2)\n",
    "        transcriptome_concat_R_sha = scale_and_transform(Roseasha_trans2, scaler2)\n",
    "        proteome_concat_R_sha = scale_and_transform(Roseasha_prot2, scaler2)\n",
    "        transcriptome_concat_MF_exp = scale_and_transform(MFexp_trans2, scaler2)\n",
    "        proteome_concat_MF_exp = scale_and_transform(MFexp_prot2, scaler2)\n",
    "        transcriptome_concat_R_exp = scale_and_transform(Roseaexp_trans2, scaler2)\n",
    "        proteome_concat_R_exp = scale_and_transform(Roseaexp_prot2, scaler2)\n",
    "\n",
    "        # Feature selection using PCA\n",
    "        most_important1_MF_sha = get_important_features(transcriptome_concat_MF_sha)\n",
    "        most_important2_MF_sha = get_important_features(proteome_concat_MF_sha)\n",
    "        most_important1_MF_exp = get_important_features(transcriptome_concat_MF_exp)\n",
    "        most_important2_MF_exp = get_important_features(proteome_concat_MF_exp)\n",
    "        most_important1_R_sha = get_important_features(transcriptome_concat_R_sha)\n",
    "        most_important2_R_sha = get_important_features(proteome_concat_R_sha)\n",
    "        most_important1_R_exp = get_important_features(transcriptome_concat_R_exp)\n",
    "        most_important2_R_exp = get_important_features(proteome_concat_R_exp)\n",
    "\n",
    "        # Consolidating important features\n",
    "        important_1 = list(set(most_important1_MF_sha + most_important1_R_sha + most_important1_R_exp + most_important1_MF_exp))\n",
    "        important_2 = list(set(most_important2_MF_sha + most_important2_R_sha + most_important2_MF_exp + most_important2_R_exp))\n",
    "\n",
    "        # Subsetting DataFrames based on important features\n",
    "        dataframes = {\n",
    "            'Roseasha_trans2': Roseasha_trans2,\n",
    "            'MFsha_trans2': MFsha_trans2,\n",
    "            'Roseasha_prot2': Roseasha_prot2,\n",
    "            'MFsha_prot2': MFsha_prot2,\n",
    "            'Roseaexp_trans2': Roseaexp_trans2,\n",
    "            'MFexp_trans2': MFexp_trans2,\n",
    "            'Roseaexp_prot2': Roseaexp_prot2,\n",
    "            'MFexp_prot2': MFexp_prot2\n",
    "        }\n",
    "\n",
    "        # Apply column filtering based on the DataFrame name\n",
    "        for name, df in dataframes.items():\n",
    "            if 'trans' in name:\n",
    "                dataframes[name] = df.iloc[:, important_1]\n",
    "            else:\n",
    "                dataframes[name] = df.iloc[:, important_2]\n",
    "\n",
    "        # Now you can access the modified DataFrames\n",
    "        Roseasha_trans2 = dataframes['Roseasha_trans2']\n",
    "        MFsha_trans2 = dataframes['MFsha_trans2']\n",
    "        Roseasha_prot2 = dataframes['Roseasha_prot2']\n",
    "        MFsha_prot2 = dataframes['MFsha_prot2']\n",
    "        Roseaexp_trans2 = dataframes['Roseaexp_trans2']\n",
    "        MFexp_trans2 = dataframes['MFexp_trans2']\n",
    "        Roseaexp_prot2 = dataframes['Roseaexp_prot2']\n",
    "        MFexp_prot2 = dataframes['MFexp_prot2']\n",
    "\n",
    "        scipy.sparse.csc_matrix.A = property(lambda self: self.toarray())\n",
    "        Roseasha_trans3 = sort_and_concat(Roseasha_trans2, cam_tran_Rosea_sha)\n",
    "        MFsha_trans3 = sort_and_concat(MFsha_trans2, cam_tran_MF_sha)\n",
    "        Roseasha_prot3 = sort_and_concat(Roseasha_prot2, cam_prot_Rosea_sha)\n",
    "        MFsha_prot3 = sort_and_concat(MFsha_prot2, cam_prot_MF_sha)\n",
    "\n",
    "        Roseaexp_trans3 = sort_and_concat(Roseaexp_trans2, cam_tran_Rosea_exp)\n",
    "        MFexp_trans3 = sort_and_concat(MFexp_trans2, cam_tran_MF_exp)\n",
    "        Roseaexp_prot3 = sort_and_concat(Roseaexp_prot2, cam_prot_Rosea_exp)\n",
    "        MFexp_prot3 = sort_and_concat(MFexp_prot2, cam_prot_MF_exp)\n",
    "\n",
    "        # Sort index for additional DataFrames if needed\n",
    "        MFsha_met = MFsha_met.sort_index(axis=1)\n",
    "        Roseasha_met = Roseasha_met.sort_index(axis=1)\n",
    "\n",
    "        MFexp_met = MFexp_met.sort_index(axis=1)\n",
    "        Roseaexp_met = Roseaexp_met.sort_index(axis=1)\n",
    "        \n",
    "        features_tran = list(Roseasha_trans3.iloc[:12, :].columns)\n",
    "        features_prot = list(Roseasha_prot3.iloc[:12, :].columns)\n",
    "        features_met = list(met_names)\n",
    "        features = features_tran + features_prot + features_met\n",
    "        features2 = [i + \"_transcript\" for i in features_tran] + [i + \"_protein\" for i in features_prot] + features_met\n",
    "\n",
    "        names = list(np.repeat(\"NA\", len(features)))\n",
    "        names2 = list(np.repeat(\"NA\", len(features)))\n",
    "\n",
    "        products = list(np.repeat(\"NA\", len(features)))\n",
    "        for k2, val1 in enumerate(features):\n",
    "            for j2, val2 in enumerate(feature_labels['SingleCopyOG']):\n",
    "                if val2 == val1:\n",
    "                    names[k2] = str(feature_labels['Name'].iloc[j2]) + ' (' + features2[k2] + ')'\n",
    "                    names2[k2] = str(feature_labels['Name'].iloc[j2]) \n",
    "                    products[k2] = str(feature_labels['Product'].iloc[j2]) + ' (' + features2[k2] + ')'\n",
    "                elif val1 in features_met:\n",
    "                    names[k2] = val1\n",
    "                    names2[k2] = val1\n",
    "                    products[k2] = val1\n",
    "\n",
    "        products = [re.sub(\"%2C\", \"\", x) for x in products]\n",
    "        names = [re.sub(\"%2C\", \"\", x) for x in names]\n",
    "        names2 = [re.sub(\"%2C\", \"\", x) for x in names2]\n",
    "    \n",
    "        timepoint_data = [\n",
    "        (MFsha_trans3, MFsha_prot3, MFsha_met, Malate_MF_sha, CAM_pheno_M_sha, MFgas),\n",
    "        (MFexp_trans3, MFexp_prot3, MFexp_met, Malate_MF_exp, CAM_pheno_M_exp, MFgas),\n",
    "        (Roseasha_trans3, Roseasha_prot3, Roseasha_met, Malate_Rosea_sha, CAM_pheno_R_sha, Roseagas),\n",
    "        (Roseaexp_trans3, Roseaexp_prot3, Roseaexp_met, Malate_Rosea_exp, CAM_pheno_R_exp, Roseagas) ]\n",
    "        \n",
    "        data_sets = [timepoint_rearrangements(data1=d[0], data2=d[1], data3=d[2], output1=d[3], output2=d[4], output3=d[5]) for d in tuple(timepoint_data)]\n",
    "\n",
    "        # Unpack the results\n",
    "        (train1_X_M_sha, train2_X_M_sha, train3_X_M_sha, train1_Y_M_sha, train2_Y_M_sha, train3_Y_M_sha, Pheno_t1_Multi_sha, Pheno_t0_Multi_sha), \\\n",
    "        (train1_X_M_exp, train2_X_M_exp, train3_X_M_exp, train1_Y_M_exp, train2_Y_M_exp, train3_Y_M_exp, Pheno_t1_Multi_exp, Pheno_t0_Multi_exp), \\\n",
    "        (train1_X_R_sha, train2_X_R_sha, train3_X_R_sha, train1_Y_R_sha, train2_Y_R_sha, train3_Y_R_sha, Pheno_t1_Rosea_sha, Pheno_t0_Rosea_sha), \\\n",
    "        (train1_X_R_exp, train2_X_R_exp, train3_X_R_exp, train1_Y_R_exp, train2_Y_R_exp, train3_Y_R_exp, Pheno_t1_Rosea_exp, Pheno_t0_Rosea_exp) = data_sets\n",
    "\n",
    "        # Evaluate kernel with noisy data\n",
    "        grid, control_sensitivity, G1, G2, cd, red_feat_space1, red_feat_space2 = eval_kernel(\n",
    "            input_X1=[train1_X_M_sha, train1_X_M_exp, train1_X_R_sha, train1_X_R_exp],\n",
    "            input_X2=[train2_X_M_sha, train2_X_M_exp, train2_X_R_sha, train2_X_R_exp],\n",
    "            input_X3=[train3_X_M_sha, train3_X_M_exp, train3_X_R_sha, train3_X_R_exp],\n",
    "            n_modes=6, \n",
    "            Pheno=[Pheno_t0_Multi_sha, Pheno_t0_Multi_exp, Pheno_t0_Rosea_sha, Pheno_t0_Rosea_exp],\n",
    "            Pheno_t=[Pheno_t1_Multi_sha, Pheno_t1_Multi_exp, Pheno_t1_Rosea_sha, Pheno_t1_Rosea_exp],\n",
    "            outputs=[np.concatenate((train1_Y_M_sha, train2_Y_M_sha, train3_Y_M_sha), axis=1),\n",
    "                    np.concatenate((train1_Y_M_exp, train2_Y_M_exp, train3_Y_M_exp), axis=1),\n",
    "                    np.concatenate((train1_Y_R_sha, train2_Y_R_sha, train3_Y_R_sha), axis=1),\n",
    "                    np.concatenate((train1_Y_R_exp, train2_Y_R_exp, train3_Y_R_exp), axis=1)],\n",
    "            input_Y1=[train1_Y_M_sha, train1_Y_M_exp, train1_Y_R_sha, train1_Y_R_exp],\n",
    "            input_Y2=[train2_Y_M_sha, train2_Y_M_exp, train2_Y_R_sha, train2_Y_R_exp],\n",
    "            input_Y3=[train3_Y_M_sha, train3_Y_M_exp, train3_Y_R_sha, train3_Y_R_exp],\n",
    "            input_df1=[MFsha_trans3, MFexp_trans3, Roseasha_trans3, Roseaexp_trans3],\n",
    "            input_df2=[MFsha_prot3, MFexp_prot3, Roseasha_prot3, Roseaexp_prot3],\n",
    "            input_df3=[MFsha_met, MFexp_met, Roseasha_met, Roseaexp_met],\n",
    "            comparison=[\"M_s_vs_c\", \"R_s_vs_c\", \"R_vs_M_s\", \"R_vs_M_c\"], iter=int(h), \n",
    "            products=products, names=names2, features=features, features2=names\n",
    "        )\n",
    "\n",
    "        return grid, control_sensitivity, G1, G2, names, cd, red_feat_space1, red_feat_space2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe6756",
   "metadata": {},
   "source": [
    "For execution of the script the replicate_noise function is used to iterate many times to evaluate robustness of results each time adding random noise to the input data and feature selection with no random seed during PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_noise(max_iter, task_id):\n",
    "    # Initialize dataframes\n",
    "    \n",
    "    np.random.seed(os.getpid())\n",
    "    np.random.seed(None)\n",
    "    random.seed(None)\n",
    "    \n",
    "    results = Parallel(n_jobs=32, backend=\"loky\")(delayed(run_iteration)(h, feature_labels) for h in range(max_iter))\n",
    "\n",
    "    grids = []\n",
    "    sens = []\n",
    "    G1_list = [] \n",
    "    G2_list = []\n",
    "    r2_outs = []\n",
    "    red_space1 = []\n",
    "    red_space2 = []\n",
    "\n",
    "    for result in results:\n",
    "        grid, sensitivity_matrix, G1, G2, names, cd, red_feat_space1, red_feat_space2 = result\n",
    "        grids.append(grid)\n",
    "        sens.append(sensitivity_matrix)\n",
    "        G1_list.append(G1)\n",
    "        G2_list.append(G2)\n",
    "        r2_outs.append(cd)\n",
    "        red_space1.append(red_feat_space1)\n",
    "        red_space2.append(red_feat_space2)\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    for result_df in grids:\n",
    "        # Ensure correct data types for columns\n",
    "        for column in result_df.columns:\n",
    "            result_df[column] = result_df[column].apply(convert_string_to_list)\n",
    "\n",
    "        # Append the processed DataFrame to the list\n",
    "        all_results.append(result_df)\n",
    "    \n",
    "    grids = pd.concat(all_results, ignore_index = True)\n",
    "    grids = grids.applymap(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    index = [p for p in grids.index][:int(len(grids.index)/2)] * 2\n",
    "    # result = [f\"{x}{y}\" for x, y in zip(grids['kernel'], index)]\n",
    "    # grids['kernel'] = result\n",
    "        \n",
    "    result_df = grids.groupby(['kernel', 'gamma', 'coef', 'degree', 'gamma1', 'delta1', 'coef2', 'coef3', 'l', 'retained']).agg({\n",
    "        'r2_data multi (control)': ['mean', 'std'], \n",
    "        'r2_data multi (stress)': ['mean', 'std'],\n",
    "        'r2_data rosea (control)': ['mean', 'std'],\n",
    "        'r2_data rosea (stress)': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "\n",
    "    result_df.to_csv(\"iteration_stats_all.csv\") ## data reconstruction accuracies for each kernel \n",
    "\n",
    "    sens_cleaned = [df.reset_index(drop=False) for df in sens]\n",
    "\n",
    "    sens = pd.concat(sens_cleaned, keys=[f'DF_{i}' for i in range(len(sens))], ignore_index=False)\n",
    "\n",
    "    df_mean = sens.groupby(sens.columns[0]).mean()\n",
    "    df_mean = df_mean.sort_values(df_mean.columns[0], ascending=False)\n",
    "    df_mean = df_mean[df_mean.index != \"NA\"]\n",
    "    names_1 = df_mean.index.to_series().str.replace(r'\\([^)]*?_','(', regex=True)\n",
    "    df_mean = np.array(df_mean.iloc[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    # === FIGURE SIZE ===\n",
    "    fig_width_in = 10     # adjust width in inches (between 6.68 and 19.05)\n",
    "    fig_height_in = 25    # adjust height in inches but can scale dynamically if needed\n",
    "    fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # === HEATMAP ===\n",
    "    sns.heatmap(df_mean, annot=False, cmap='coolwarm', cbar=True, yticklabels=names_1, ax=ax)\n",
    "\n",
    "    # Remove x-ticks, adjust y-ticks font size\n",
    "    plt.xticks([])\n",
    "    plt.tick_params(axis='y', labelsize=14)  \n",
    "\n",
    "    # Adjust aspect ratio for readability\n",
    "    ax.set_aspect(0.1)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label('Sensitivity values', fontsize=14)\n",
    "    # Title\n",
    "  #  plt.title('Mean Sensitivity Heatmap', fontsize=12, fontproperties=times_font)\n",
    "\n",
    "    # Tight layout with 2-point white border\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # Save as TIFF, RGB, 400 dpi\n",
    "    plt.savefig('Figure 5C.eps', format='eps', dpi=600)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # grouped eigenmode topologies - consensus eigenmode networks\n",
    "    G1_list = [G1 for G1_sub in G1_list for G1 in G1_sub]\n",
    "    G2_list = [G2 for G2_sub in G2_list for G2 in G2_sub]\n",
    "    if len(G1_list) > 1 and len(G2_list) > 1:\n",
    "\n",
    "        F1_freq, W1_sum = aggregate_edge_frequency(G1_list)\n",
    "        \n",
    "        G1_consensus = threshold_by_frequency(F1_freq, thr=0.5)\n",
    "        nodes = G1_consensus.nodes()\n",
    "        \n",
    "        A1 = nx.to_numpy_array(G1_consensus, nodelist=sorted(G1_consensus.nodes()), weight='weight', dtype=float)\n",
    "        A1 = pd.DataFrame(A1, index=names, columns=names)\n",
    "        edges = A1.reset_index().melt(id_vars='index', var_name='target', value_name='weight')\n",
    "        edges = edges.rename(columns={'index': 'source'})\n",
    "        \n",
    "        # drop self-loops and zero-weight edges\n",
    "        edges = edges[edges['weight'] != 0]\n",
    "        edges = edges[edges['source'] != edges['target']]\n",
    "        edges['abs_weight'] = np.abs(edges['weight'])\n",
    "        edges.sort_values(by=\"abs_weight\", ascending=False).iloc[:500]\n",
    "        \n",
    "        pathway_dict = {re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', str(val)).strip(): feature_labels['Pathway'].iloc[idx]\n",
    "                            for idx, val in enumerate(feature_labels['Name'])}\n",
    "            \n",
    "        product_dict = {re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', str(val)).strip(): feature_labels['Product'].iloc[idx]\n",
    "                            for idx, val in enumerate(feature_labels['Name'])}\n",
    "            \n",
    "        edges['Source_names'] = edges['source']\n",
    "        edges['Target_names'] = edges['target']\n",
    "        edges.to_csv(f'S1 Table.csv', index=False)\n",
    "\n",
    "        F2_freq, W1_sum = aggregate_edge_frequency(G2_list)\n",
    "        G2_consensus = threshold_by_frequency(F2_freq, thr=0.5)\n",
    "        nodes = G2_consensus.nodes()\n",
    "        A2 = nx.to_numpy_array(G2_consensus, nodelist=sorted(G1_consensus.nodes()), weight='weight', dtype=float)\n",
    "        A2 = pd.DataFrame(A2, index=names, columns=names)\n",
    "        edges = A2.reset_index().melt(id_vars='index', var_name='target', value_name='weight')\n",
    "        edges = edges.rename(columns={'index': 'source'})\n",
    "        \n",
    "        # drop self-loops and zero-weight edges\n",
    "        edges = edges[edges['weight'] != 0]\n",
    "        edges = edges[edges['source'] != edges['target']]\n",
    "        edges['abs_weight'] = np.abs(edges['weight'])\n",
    "        edges.sort_values(by=\"abs_weight\", ascending=False).iloc[:500]\n",
    "        edges['Source_names'] = edges['source']\n",
    "        edges['Target_names'] = edges['target']\n",
    "        edges.to_csv(f'S2 Table.csv', index=False)\n",
    "    \n",
    "    if len(r2_outs) > 1:\n",
    "        r2_outs = np.asarray(r2_outs)\n",
    "        r2_mean = np.mean(r2_outs, axis=0)\n",
    "        r2_std = np.std(r2_outs, axis=0)\n",
    "        \n",
    "        with open('r2_outs.txt', 'w') as f: ## mean for reconstruction accuracy of phenocopying Rosea outputs \n",
    "            f.write(f\"r2_outs_mean: {r2_mean}\")\n",
    "            f.write(f\"r2_outs_std: {r2_std}\")\n",
    "            f.write(f\"r2_outs_shape: {r2_outs.shape}\")\n",
    "    \n",
    "    # count how many times each feature appeared in every iteration after HSV data reduction \n",
    "    \n",
    "    combined_df1 = pd.concat(red_space1, ignore_index=True) if red_space1 else pd.DataFrame()\n",
    "    combined_df2 = pd.concat(red_space2, ignore_index=True) if red_space2 else pd.DataFrame()\n",
    "    cmap = cm.Blues\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(25, 9))  \n",
    "    norm = plt.Normalize(vmin=0, vmax=1)  # Normalize HSV values to be between 0 and 1\n",
    "    plot_species_barplot(combined_df1, axs[0], r'$\\it{C.\\ major}$', cmap, norm)  \n",
    "    plot_species_barplot(combined_df2, axs[1], r'$\\it{C.\\ rosea}$', cmap, norm)  \n",
    "    plt.subplots_adjust(right=1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Figure 5A.eps\", dpi=400, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "                \n",
    "    return \n",
    "\n",
    "reported_grid = replicate_noise(task_id='kernel_replication_1', max_iter=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
